{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f426f65-7904-4231-92a5-e899547d90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63b0fd25-41a2-48ad-b9bf-3f1265308bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b37cce2-ed84-408b-9106-d61360aa82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdaf6ce-2540-494f-989c-5b94b1b6626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b9b1f9-3c90-42b0-beb4-cb419f9cdcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57de60e5-b96c-499c-a7cf-0f30fc33b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c499838b-73b3-44be-8ba6-f46d3693aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201f0f3-b380-43a5-95ee-62b1b79002d1",
   "metadata": {},
   "source": [
    "SELECT * WHERE course = 'data-engineering-zoomcamp';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d8ea88-7412-49c1-8a8e-44d0d0862a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'the course has already started, can I still enroll?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7d0d18-5c07-4010-9f90-bbd021f110c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7b06c1bf4530>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9cb61f-5520-4739-9ebf-9ac321bbdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd463a33-ca62-4ebf-8dfc-c7a929b6566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef8e9cdc-dfd4-4e54-a332-4b9bde4e6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbecc727-c2a6-4fd5-9576-b9915dd6162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-generativeai\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a75421-a8c1-4cf1-9d91-5c64b56d542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It depends on the course.  Some courses allow late enrollment, while others do not.  You should check with the instructor or the institution offering the course to find out their policy on late enrollment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "   genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "except KeyError:\n",
    "   # Otherwise, prompt the user for the key\n",
    "   api_key = input(\"Please enter your Google API Key: \")\n",
    "   genai.configure(api_key=api_key)\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(q)\n",
    "\n",
    "try:\n",
    "    result = response.text\n",
    "    print(result)\n",
    "except ValueError:\n",
    "    print(\"Response was blocked by safety settings.\")\n",
    "    print(response.prompt_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7da9664-ecb3-4d89-87da-9b2b942444d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It depends on the policies of the institution or organization offering the course. Some courses allow late enrollment, often with certain conditions or deadlines for catching up on missed work. It's best to contact the course administrator or visit the course's website for specific information on late enrollment.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{\"role\": \"user\", \"content\": q}]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b21237c3-80e9-429c-a089-d45428087046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cc5784e-6515-42e5-be62-8fb915df1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d35dec-c25f-472d-b961-20d5c30902ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_1(prompt):\n",
    "    response_1 = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response_1.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24e04ae-51bc-42aa-b762-f0ecc330a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_2(prompt):\n",
    "    try:\n",
    "       genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "    except KeyError:\n",
    "       # Otherwise, prompt the user for the key\n",
    "       api_key = input(\"Please enter your Google API Key: \")\n",
    "       genai.configure(api_key=api_key)\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "    \n",
    "    response_2 = model.generate_content(prompt)\n",
    "    \n",
    "    try:\n",
    "        result = response_2.text\n",
    "        print(result)\n",
    "    except ValueError:\n",
    "        print(\"Response was blocked by safety settings.\")\n",
    "        print(response_2.prompt_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d75e913c-6bd7-4ef5-a75a-5fa9d24719d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I run kafka?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8602f40b-ad3b-49c9-b3cc-051a79c888bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_1(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer_1 = llm_1(prompt)\n",
    "    return answer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a47162a-65d3-4050-866a-098155deaa53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Kafka using Java in the terminal, navigate to the project directory and execute the following command:\\n\\n```bash\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n```\\n\\nFor running Python Kafka scripts in a virtual environment, create a virtual environment, activate it, and install the necessary packages:\\n\\n1. Create and activate the virtual environment (run only once):\\n   ```bash\\n   python -m venv env\\n   source env/bin/activate\\n   pip install -r ../requirements.txt\\n   ```\\n\\n2. Activate the virtual environment each time you need it:\\n   ```bash\\n   source env/bin/activate\\n   ```\\n\\n3. Deactivate the virtual environment when done:\\n   ```bash\\n   deactivate\\n   ```\\n\\nEnsure Docker images are up and running before using the virtual environment for Python scripts.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_1(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9347ba19-663c-4e4d-807d-1f9640cfeaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_2(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer_2 = llm_2(prompt)\n",
    "    return answer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fd4497b-c5d5-4258-b950-6b35d1af4ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text gives instructions for running Kafka producers and consumers in Java and Python.  For Java, run `java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java` from the project directory.  For Python, first create a virtual environment using `python -m venv env`, then activate it using `source env/bin/activate` (or `env/Scripts/activate` on Windows), install dependencies with `pip install -r ../requirements.txt`, and then run the Python files.  If you encounter a \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\" error, use `pip install kafka-python-ng` instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "385b012f-4905-422d-8d7c-3d542dfe5a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, you can still enroll in the course after it has started. You are eligible to submit the homework even without registering. However, please be mindful of the deadlines for turning in the final projects and try not to leave everything until the last minute.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_1('the course has already started, can I still enroll?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa515120-3412-4c04-aa6c-75479ff97d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still submit homeworks even if you don't register.  However, be aware of the deadlines for final projects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_2('the course has already started, can I still enroll?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3e04fb3-b7f7-4e53-8de9-a1c6cde3f038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c05052f-a85a-4137-8398-0fd0be678599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b99aef11-31ca-4a1d-baa0-db2ae9462b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23f927f2-44bd-40de-b782-7bc81424dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a78df1cc-5a5a-40b4-b673-19c7f0319453",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a1ca6ef-7459-4df2-8dc2-8d6e750cd777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'course-questions' already exists\n",
      "✓ Index 'course-questions' is ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Check if index exists, create if it doesn't\n",
    "if es_client.indices.exists(index=index_name):\n",
    "    print(f\"Index '{index_name}' already exists\")\n",
    "else:\n",
    "    es_client.indices.create(\n",
    "        index=index_name,\n",
    "        settings=index_settings[\"settings\"],\n",
    "        mappings=index_settings[\"mappings\"]\n",
    "    )\n",
    "    print(f\"Index '{index_name}' created successfully!\")\n",
    "\n",
    "# Verify it was created\n",
    "if es_client.indices.exists(index=index_name):\n",
    "    print(f\"✓ Index '{index_name}' is ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f778c93-a5b6-4634-b42e-0c25083a2512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c230059-e219-4a13-a7f8-ede4cf1b028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70fe3c97-916d-42c0-bd7b-4f42d9056409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8821d7fe384c2595bcccef28a4e6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1bc1244-b8dc-4228-8171-c0507004db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_query = 'How do execute a command on a Kubernetes pod?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c72e000-910b-4fb5-aa88-2561e7bc39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"data-engineering-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81abecbc-eb6b-428f-ab7d-7e21f58b64de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_1(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer_1 = llm_1(prompt)\n",
    "    return answer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "871a165b-89c3-461c-82c9-f91cc3436c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_2(query):\n",
    "    search_results = elastic_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer_2 = llm_2(prompt)\n",
    "    return answer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ea9315a-a619-4066-9e90-8c260f2c8450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided CONTEXT does not include information on executing a command on a Kubernetes pod. Please refer to Kubernetes documentation or other reliable sources for guidance on how to perform this action.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_1(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e809a53-6b0f-4571-8ccd-295aab31b230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This question cannot be answered from the given context.  The provided text focuses on using dbt-core projects in Airflow and handling type errors in PySpark, but it does not contain information about executing commands on Kubernetes pods.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_2(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eb235e6-0721-4cd8-b348-6cc1656c597e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "  'section': 'Course Management Form for Homeworks',\n",
       "  'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "  'section': 'Course Management Form for Homeworks',\n",
       "  'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "  'section': 'Course Management Form for Homeworks',\n",
       "  'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Install the astronomer-cosmos package as a dependency. (see Terraform example).\\nMake a new folder, dbt/, inside the dags/ folder of your Composer GCP bucket and copy paste your dbt-core project there. (see example)\\nEnsure your profiles.yml is configured to authenticate with a service account key. (see BigQuery example)\\nCreate a new DAG using the DbtTaskGroup class and a ProfileConfig specifying a profiles_yml_filepath that points to the location of your JSON key file. (see example)\\nYour dbt lineage graph should now appear as tasks inside a task group like this:',\n",
       "  'section': 'Course Management Form for Homeworks',\n",
       "  'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'TypeError when using spark.createDataFrame function on a pandas df',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Error:\\nspark.createDataFrame(df_pandas).schema\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\nSolution:\\nAffiliated_base_number is a mix of letters and numbers (you can check this with a preview of the table), so it cannot be set to DoubleType (only for double-precision numbers). The suitable type would be StringType. Spark  inferSchema is more accurate than Pandas infer type method in this case. You can set it to  true  while reading the csv, so you don’t have to take out any data from your dataset. Something like this can help:\\ndf = spark.read \\\\\\n.options(\\nheader = \"true\", \\\\\\ninferSchema = \"true\", \\\\\\n) \\\\\\n.csv(\\'path/to/your/csv/file/\\')\\nSolution B:\\nIt\\'s because some rows in the affiliated_base_number are null and therefore it is assigned the datatype String and this cannot be converted to type Double. So if you really want to convert this pandas df to a pyspark df only take the  rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column. Then you will be able to apply the pyspark function createDataFrame.\\n# Only take rows that have no null values\\npandas_df= pandas_df[pandas_df.notnull().all(1)]',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'TypeError when using spark.createDataFrame function on a pandas df',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_search(es_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cf21121-187a-4f58-a51f-c71ad4564f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m top_result = elastic_search(query)\n\u001b[32m      2\u001b[39m top_result[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop result (score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtop_result\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscore\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(top_result[\u001b[33m'\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Print all results with their scores\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "top_result = elastic_search(query)\n",
    "top_result[0]\n",
    "print(f\"Top result (score: {top_result['score']}):\")\n",
    "print(top_result['document'])\n",
    "\n",
    "# Print all results with their scores\n",
    "print(\"\\nAll results:\")\n",
    "for result in results_with_scores:\n",
    "    print(f\"Rank {result['rank']}: Score {result['score']:.2f}\")\n",
    "    print(f\"Question: {result['document']['question']}\")\n",
    "    print(f\"Section: {result['document']['section']}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
